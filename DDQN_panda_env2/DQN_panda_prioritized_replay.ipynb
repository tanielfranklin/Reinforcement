{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação DQN com prioritized replay no ambiente Swift com manipulador panda.\n",
    "\n",
    "Replay buffer <br>\n",
    "DNN para representar o agente atual e uma DNN alvo com uma taxa de atualização menor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "from scipy.signal import convolve, gaussian\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import time\n",
    "import glob\n",
    "from IPython.display import HTML\n",
    "import data_panda as rbt\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#j3 range -0.08 a 3.75  #j2 range -0.07 a -3. #j1 range -1.8 a 1.76\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sistema observável e com medidas dos ângulos disponíveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Observation:\n",
    "        Type: Box(4)\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Joint1                   -4.8                    4.8\n",
    "        1       Joint2                    -Inf                    Inf\n",
    "        2       Joint3                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "        \n",
    "    Actions:\n",
    "        Type: Discrete(9)\n",
    "        Num   Three actions for each joint\n",
    "        0     decrement joint j\n",
    "        1     increment joint j\n",
    "        2     decrement join  j\n",
    "\n",
    "        #j3 range 0.0 a 3.7\n",
    "        #j2 range 0.0 a -3.\n",
    "        #j1 range -1.7 a 1.7\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.panda = rp.models.Panda()\n",
    "# self.panda_end = rp.models.Panda()\n",
    "# self.m=100 #magnification factor\n",
    "\n",
    "# self.obstacle = Cuboid([0.2, 0.2, 0.8], pose=sm.SE3(0.3, 0, 0)) \n",
    "# #self.floor = Cuboid([0.2, 0.2, 0.8], pose=sm.SE3(-0.2, 0, 0)) \n",
    "# self.obs_floor = Cuboid([2., 2., 0.01], pose=sm.SE3(0, 0, 0), color=[100,100,100,0])#\"black\") #color=[100,100,100,0]\n",
    "# self.scene.add(self.obs_floor)\n",
    "# self.scene.add(self.obstacle)\n",
    "# self.scene.add(self.panda, robot_alpha=0.6)\n",
    "# self.delta=delta\n",
    "# #End joints positions\n",
    "# j=[0.8,-1.5,1] \n",
    "# self.q_end=[0., j[0], 0.,j[1], 0., j[2], 0.]\n",
    "# self.Tep = self.panda.fkine(self.q_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shape = 3\n",
    "env=rbt.Panda_RL()\n",
    "agent=rbt.DQNAgent(state_shape, epsilon=0).to(device)\n",
    "env.delta=0.02\n",
    "RESTORE_AGENT=False # Restore a trained agent\n",
    "NEW_BUFFER=True # Restore a buffer\n",
    "TRAIN=True # Train or only simulate\n",
    "env.renderize=True #stop robot viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERobot: panda (by Franka Emika), 7 joints (RRRRRRR), 1 gripper, geometry, collision\n",
       "┌─────┬──────────────┬───────┬─────────────┬────────────────────────────────────────────────┐\n",
       "│link │     link     │ joint │   parent    │              ETS: parent to link               │\n",
       "├─────┼──────────────┼───────┼─────────────┼────────────────────────────────────────────────┤\n",
       "│   0 │ panda_link0  │       │ BASE        │                                                │\n",
       "│   1 │ panda_link1  │     0 │ panda_link0 │ SE3(0, 0, 0.333) ⊕ Rz(q0)                      │\n",
       "│   2 │ panda_link2  │     1 │ panda_link1 │ SE3(-90°, -0°, 0°) ⊕ Rz(q1)                    │\n",
       "│   3 │ panda_link3  │     2 │ panda_link2 │ SE3(0, -0.316, 0; 90°, -0°, 0°) ⊕ Rz(q2)       │\n",
       "│   4 │ panda_link4  │     3 │ panda_link3 │ SE3(0.0825, 0, 0; 90°, -0°, 0°) ⊕ Rz(q3)       │\n",
       "│   5 │ panda_link5  │     4 │ panda_link4 │ SE3(-0.0825, 0.384, 0; -90°, -0°, 0°) ⊕ Rz(q4) │\n",
       "│   6 │ panda_link6  │     5 │ panda_link5 │ SE3(90°, -0°, 0°) ⊕ Rz(q5)                     │\n",
       "│   7 │ panda_link7  │     6 │ panda_link6 │ SE3(0.088, 0, 0; 90°, -0°, 0°) ⊕ Rz(q6)        │\n",
       "│   8 │ @panda_link8 │       │ panda_link7 │ SE3(0, 0, 0.107)                               │\n",
       "└─────┴──────────────┴───────┴─────────────┴────────────────────────────────────────────────┘\n",
       "\n",
       "┌─────┬─────┬────────┬─────┬───────┬─────┬───────┬──────┐\n",
       "│name │ q0  │ q1     │ q2  │ q3    │ q4  │ q5    │ q6   │\n",
       "├─────┼─────┼────────┼─────┼───────┼─────┼───────┼──────┤\n",
       "│  qr │  0° │ -17.2° │  0° │ -126° │  0° │  115° │  45° │\n",
       "│  qz │  0° │  0°    │  0° │  0°   │  0° │  0°   │  0°  │\n",
       "└─────┴─────┴────────┴─────┴───────┴─────┴───────┴──────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env.panda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_network = rbt.DQNAgent(agent.state_shape, epsilon=0.5).to(device)\n",
    "#Copying weights from agent network\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb70011a6d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.panda.q=env.panda.qz\n",
    "# set a seed\n",
    "seed = 13\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.245333459951897 2.8228782900000007\n"
     ]
    }
   ],
   "source": [
    "q_far=np.array([ 0., -0.8 ,  0. , -0.0698,  0.,  3.3825,  0.    ])\n",
    "env.panda.q=q_far\n",
    "env.scene.step()\n",
    "print(env.distance(),env.fitness())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0643562098777974 14.428072040000002\n"
     ]
    }
   ],
   "source": [
    "env.panda.q=env.panda.qz\n",
    "env.scene.step()\n",
    "print(env.distance(),env.fitness())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.842994640253499 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  -0.6095   -0.6095    0.5069    0.6235    \n",
       "   0.7071   -0.7071    0         0         \n",
       "   0.3584    0.3584    0.862     0.9592    \n",
       "   0         0         0         1         \n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.panda.q=env.q_goal\n",
    "env.scene.step()\n",
    "print(env.distance(),env.fitness())\n",
    "T_end=env.panda.fkine(env.q_goal)\n",
    "T_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current number of result directories: 0\n",
      "runs/results_1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'runs/results_1'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_res_dir():\n",
    "    # Directory to store results\n",
    "    res_dir_count = len(glob.glob('runs/*'))\n",
    "    print(f\"Current number of result directories: {res_dir_count}\")\n",
    "    if TRAIN:\n",
    "        RES_DIR = f\"runs/results_{res_dir_count+1}\"\n",
    "        print(RES_DIR)\n",
    "    else:\n",
    "        RES_DIR = f\"runs/results_{res_dir_count}\"\n",
    "    return RES_DIR\n",
    "\n",
    "set_res_dir()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill buffer with samples collected ramdomly from environment\n",
    "buffer_len=5000\n",
    "tmax=800\n",
    "env.renderize=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "env.renderize=False\n",
    "exp_replay = rbt.PrioritizedReplayBuffer(buffer_len)\n",
    "if NEW_BUFFER:\n",
    "    \n",
    "    \n",
    "    for i in trange(60):\n",
    "        \n",
    "        state=env.reset()\n",
    "        # Play 100 runs of experience with 100 steps and  stop if reach 10**4 samples\n",
    "        rbt.play_and_record(state, agent, env, exp_replay, n_steps=60)\n",
    "        \n",
    "        if len(exp_replay) == buffer_len:\n",
    "            break\n",
    "    print(len(exp_replay))\n",
    "\n",
    "\n",
    "\n",
    "    with open('buffer.pickle', 'wb') as handle:\n",
    "        pickle.dump(exp_replay.buffer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "else:\n",
    "    exp_replay = rbt.PrioritizedReplayBuffer(buffer_len)\n",
    "    with open('buffer.pickle', 'rb') as handle:\n",
    "        exp_replay.buffer=pickle.load(handle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def monitor_tensorboard():\n",
    "#     %reload_ext tensorboard\n",
    "#     %tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current number of result directories: 0\n",
      "runs/results_1\n"
     ]
    }
   ],
   "source": [
    "RES_DIR = set_res_dir()\n",
    "comment=\"\"\n",
    "# monitor_tensorboard()\n",
    "tb=SummaryWriter(log_dir=RES_DIR, comment=comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_of_total_steps=0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup some parameters for training\n",
    "timesteps_per_epoch = 1\n",
    "batch_size = 64\n",
    "total_steps = 40* 10**3\n",
    "#total_steps = 10\n",
    "\n",
    "#init Optimizer\n",
    "lr=1e-4\n",
    "opt = torch.optim.Adam(agent.parameters(), lr=lr)\n",
    "\n",
    "# set exploration epsilon \n",
    "start_epsilon = 1\n",
    "start_epsilon = 0.1\n",
    "end_epsilon = 0.05\n",
    "eps_decay_final_step = percentage_of_total_steps*total_steps\n",
    "\n",
    "# setup some frequency for logging and updating target network\n",
    "loss_freq = 40\n",
    "refresh_target_network_freq = 50\n",
    "eval_freq = 1000\n",
    "\n",
    "# to clip the gradients\n",
    "max_grad_norm = 5000\n",
    "\n",
    "hyperparameters_train={\"start_epsilon\":start_epsilon,\n",
    "                        \"end_epsilon\":end_epsilon,\n",
    "                        \"lr\":lr,\n",
    "                        \"batch_size\":batch_size,\n",
    "                        \"total_steps\":total_steps,\n",
    "                        \"percentage_of_total_steps\":percentage_of_total_steps,\n",
    "                        \"refresh_target_network_freq\":refresh_target_network_freq,\n",
    "                        \"buffer_len\": buffer_len,\n",
    "                        \"tmax\":tmax, \n",
    "                        \"agent\":str(agent.network) \n",
    "                        }\n",
    "def save_hyperparameter(dict,directory):    \n",
    "    with open(directory+\"/hyperparameters.json\", \"w\") as outfile:\n",
    "        json.dump(hyperparameters_train, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40001 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "env.renderize=False\n",
    "if TRAIN:\n",
    "    \n",
    "    state = env.reset()\n",
    "    tb.add_graph(agent.network, torch.tensor(state, device=device, dtype=torch.float32))\n",
    "    save_hyperparameter(hyperparameters_train,RES_DIR)\n",
    "    loss_min=np.inf\n",
    "    \n",
    "    for step in trange(total_steps + 1):\n",
    "        \n",
    "        \n",
    "        # reduce exploration as we progress\n",
    "        agent.epsilon = rbt.epsilon_schedule(start_epsilon, end_epsilon, step, eps_decay_final_step)\n",
    "\n",
    "        # take timesteps_per_epoch and update experience replay buffer\n",
    "        _, state = rbt.play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "        \n",
    "        # train by sampling batch_size of data from experience replay\n",
    "        states, actions, rewards, next_states, done_flags, weights, idxs = exp_replay.sample(batch_size)\n",
    "        actions =[agent.get_action_index(i) for i in actions]\n",
    "        \n",
    "\n",
    "        # loss = <compute TD loss>\n",
    "\n",
    "        loss = rbt.compute_td_loss_priority_replay(agent, target_network, exp_replay,\n",
    "                           states, actions, rewards, next_states, done_flags, weights, idxs,              \n",
    "                           gamma=0.99,\n",
    "                           device=device)\n",
    "        if loss<loss_min:\n",
    "            torch.save(agent.state_dict(), RES_DIR+'/best-model.pt')\n",
    "         \n",
    "        tb.add_scalar(\"1/TD Loss\", loss, step)\n",
    "        loss.backward()\n",
    "        grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        tb.add_scalar(\"2/Epsilon\", agent.epsilon,step)\n",
    "\n",
    "        if step % refresh_target_network_freq == 0:\n",
    "            # Load agent weights into target_network\n",
    "            target_network.load_state_dict(agent.state_dict())\n",
    "\n",
    "        if step % eval_freq == 0:\n",
    "            # eval the agent\n",
    "            m_reward=rbt.evaluate(env, agent, n_games=50, greedy=True, t_max=tmax)[0] \n",
    "            tb.add_scalar(\"1/Mean reward per episode\", m_reward, step)\n",
    "            clear_output(True)\n",
    "            print(\"buffer size = %i, epsilon = %.5f\" %\n",
    "                (len(exp_replay), agent.epsilon))\n",
    "            print(f\"Frequency evaluation = {eval_freq}\")\n",
    "\n",
    "    torch.save(agent.state_dict(), RES_DIR+'/last-model.pt')        \n",
    "    tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score:363.72938874160013 in 299.0 steps\n",
      "[['', '']]\n",
      "Well done , Distance: 0.611573877831515, Fitness: 0.014699999999999842\n",
      "collision False\n"
     ]
    }
   ],
   "source": [
    "env.renderize=True\n",
    "final_score,m_steps,infos = rbt.evaluate(env,agent, n_games=1, greedy=True, t_max=300)\n",
    "print(f'final score:{final_score} in {m_steps} steps')\n",
    "print(infos)\n",
    "print(f'Well done , Distance: {env.distance()}, Fitness: {env.fitness()}')\n",
    "print(f\"collision {env.detect_collision()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_AGENT=False\n",
    "SAVE_AGENT=False\n",
    "\n",
    "FILE_PREFIX=['mean_rw_history','td_loss_history']\n",
    "EXTENSION='.pickle'\n",
    "NAME=\"-reduced_network\"\n",
    "FILENAME=[]\n",
    "for i in FILE_PREFIX:\n",
    "    FILENAME.append(i+NAME+EXTENSION)\n",
    "\n",
    "\n",
    "if SAVE_AGENT:\n",
    "    torch.save(agent.state_dict(), 'model_panda_trained.pth')\n",
    "    for (i,j) in zip(FILENAME,FILE_PREFIX):\n",
    "        with open(i, 'wb') as handle:\n",
    "            pickle.dump(j, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15 -0.55  3.2 ] -20.706927372722966 False ['', '']\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "qvalues = agent.get_qvalues([state])\n",
    "action = agent.actions_space[qvalues.argmax(axis=-1)[0]]\n",
    "state, r, done, info = env.step(action)\n",
    "print(state, r, done, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qvalues = agent.get_qvalues([state])\n",
    "action = agent.actions_space[qvalues.argmax(axis=-1)[0]]\n",
    "state, r, done, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got reward: -62.69144567753095\n",
      "Done , Distance: 0.6163060183056325\n",
      "collision False\n",
      "['', '']\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "reward = 0\n",
    "while True:\n",
    "    qvalues = agent.get_qvalues([state])\n",
    "    action = agent.actions_space[qvalues.argmax(axis=-1)[0]]\n",
    "    state, r, done, info = env.step(action)\n",
    "    reward += r\n",
    "    #print(reward)\n",
    "    if done or reward < -60:\n",
    "        print('Got reward: {}'.format(reward))\n",
    "        break\n",
    "print(f'Done , Distance: {env.distance()}')\n",
    "print(f\"collision {env.detect_collision()[0]}\")\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[339.97763, 352.16226, 341.89145, 349.60864, 346.29236, 349.448  ,\n",
       "        352.5781 , 350.36682, 354.35278, 351.50418, 355.7459 , 350.91345,\n",
       "        358.81012, 357.651  , 356.27682, 351.90414, 357.14294, 312.89474,\n",
       "        347.17572, 345.7454 , 350.4716 , 351.26053, 357.87143, 354.52103,\n",
       "        357.70673, 347.29922]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state=env.get_q()\n",
    "qvalues = agent.get_qvalues([state])\n",
    "qvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us record a video of trained agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Animate learned policy\n",
    "# save_dir='./videos/'\n",
    "# #env = make_env(env_name)\n",
    "# generate_animation(env, agent, save_dir=save_dir)\n",
    "# [filepath] = glob.glob(os.path.join(save_dir, '*.mp4'))\n",
    "\n",
    "# display_animation(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "# Run this on another environment in OpenAI Gym\n",
    "# Create a robotic environment with more actions\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('RTB')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "007c3d4ec7d1faff975b7b2e6628fd4907f6779597816faa5595ad19bf6f4797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
